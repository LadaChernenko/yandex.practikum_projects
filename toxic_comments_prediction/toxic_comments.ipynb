{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация токсичных комментариев\n",
    "Интернет-магазин запускает новый сервис. Теперь пользователи сами смогут редактировать описания товаров. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Необходимо обучить модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План выполнения работы:\n",
    "### <a href=#1>1. Подготовка</a>\n",
    "-    #### <a href=#1_1> 1.1 Очистка и лемматизация текста</a>\n",
    "-    #### <a href=#1_2> 1.2 Подготовка выборок</a>\n",
    "-    #### <a href=#1_3> 1.3 Мешок слов</a>\n",
    "-    #### <a href=#1_4> 1.4 TF-IDF</a>\n",
    "-    #### <a href=#1_5> 1.5 word2vec</a>\n",
    "\n",
    "### <a href=#2>2. Обучение</a>\n",
    "\n",
    "### <a href=#3>3. Выводы</a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'> 1. Подготовка</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "import re \n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1_1'> 1.1 Очистка и лемматизация текста</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#очистка от стопслов\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "def preprocess_news(text):\n",
    "\n",
    "    #очистка текста\n",
    "    words = \" \".join((re.sub(r'[^a-zA-Z ]', ' ', text)).split())\n",
    "    #создание токенов\n",
    "    words = [w for w in nltk.word_tokenize(words) if (w not in stopwords)]\n",
    "    #лемматизация\n",
    "    words = [lem.lemmatize(w.lower(), pos=\"v\") for w in words]\n",
    "\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#корпус постов\n",
    "toxic_comments['corpus'] = preprocess_news(toxic_comments['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = toxic_comments['text'].apply(preprocess_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    d aww he match background colour i seemingly s...\n",
       "2    hey man i really try edit war it guy constantl...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1_2'> 1.2 Подготовка выборок</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = toxic_comments.loc[:,'toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобъём выборку на тестовые и тренировочные части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер X_train:   127656  \n",
      "Размер y_train:   127656   \n",
      "Размер X_test:    31915   \n",
      "Размер y_test:    31915    \n"
     ]
    }
   ],
   "source": [
    "print('Размер X_train: {:^10}'.format(X_train.shape[0]))\n",
    "print('Размер y_train: {:^10} '.format(y_train.shape[0]))\n",
    "print('Размер X_test:  {:^10}'.format(X_test.shape[0]))\n",
    "print('Размер y_test:  {:^10} '.format(y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1_3'> 1.3 Мешок слов</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#мешок слов с стоп-словами\n",
    "count_vect = CountVectorizer(stop_words=stopwords)\n",
    "\n",
    "X_bof_train = count_vect.fit_transform(X_train)\n",
    "X_bof_test = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер X_bof_train:   127656     137023   \n",
      "Размер X_bof_test:    31915      137023   \n"
     ]
    }
   ],
   "source": [
    "print('Размер X_bof_train: {:^10} {:^10} '.format(X_bof_train.shape[0], X_bof_train.shape[1]))\n",
    "print('Размер X_bof_test:  {:^10} {:^10} '.format(X_bof_test.shape[0], X_bof_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1_4'> 1.4 TF-IDF</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF со стопсловами\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "X_tfidf_train = count_tf_idf.fit_transform(X_train) \n",
    "X_tfidf_test = count_tf_idf.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер X_tfidf_train:   127656     137023   \n",
      "Размер X_tfidf_test:    31915      137023   \n"
     ]
    }
   ],
   "source": [
    "print('Размер X_tfidf_train: {:^10} {:^10} '.format(X_tfidf_train.shape[0], X_tfidf_train.shape[1]))\n",
    "print('Размер X_tfidf_test:  {:^10} {:^10} '.format(X_tfidf_test.shape[0], X_tfidf_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1_5'> 1.5 word2vec</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(size=300, min_count=1, window = 10, alpha=0.03)\n",
    "w2v_model.build_vocab(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "w2v_model.train(X_train, total_examples=w2v_model.corpus_count, epochs=w2v_model.iter)\n",
    "w2v = dict(zip(w2v_model.wv.index2word, w2v_model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#найдём векторное представление каждого твита\n",
    "def mean_transform(X):\n",
    "    return np.array([np.mean([w2v[w] for w in words if w in w2v]\n",
    "                             or [np.zeros(300)], axis=0)for words in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_train = mean_transform(X_train)\n",
    "X_w2v_test = mean_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер X_w2v_train:   127656      300     \n",
      "Размер X_w2v_test:    31915       300     \n"
     ]
    }
   ],
   "source": [
    "print('Размер X_w2v_train: {:^10} {:^10} '.format(X_w2v_train.shape[0], X_w2v_train.shape[1]))\n",
    "print('Размер X_w2v_test:  {:^10} {:^10} '.format(X_w2v_test.shape[0], X_w2v_test.shape[1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "toxic_comments['word_2_vec']=word_2_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='2'> 2. Обучение</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_features = {'bag_of_words': [X_bof_train, X_bof_test, y_train, y_test],\n",
    "                'TF-IDF': [X_tfidf_train, X_tfidf_test, y_train, y_test],\n",
    "                'word2vec': [X_w2v_train, X_w2v_test, y_train, y_test]}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_best_params(X_train, y_train, model, parameters):\n",
    "\n",
    "    #Подберём лучшие параметры модели с помощью RandomizedSearchCV\n",
    "    clf = RandomizedSearchCV(estimator = model,\n",
    "                             param_distributions = parameters,\n",
    "                             scoring='f1', cv = 5,\n",
    "                             random_state = 124)\n",
    "    search = clf.fit(X_train, y_train)\n",
    "    print('Best Params: ', search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для предсказаний и вычисления метрики качества\n",
    "def get_f1(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = f1_score(y_test, y_pred, average='binary')\n",
    "    print('F1: {:.4f}'.format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве моделей возьмём `LogisticRegression`, `LinearSVC` и `LGBMRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7328\n",
      "F1: 0.7610\n",
      "F1: 0.3504\n"
     ]
    }
   ],
   "source": [
    "log_reg_score=[] #сюда будем записывать метрику качества модели логистической регрессии\n",
    "log_regres_model = LogisticRegression(C=2.5,\n",
    "                                      class_weight='balanced', \n",
    "                                      random_state=42)\n",
    "for key in vec_features.keys():\n",
    "    log_regres_model.fit(vec_features[key][0], vec_features[key][2])\n",
    "    score = get_f1(log_regres_model, vec_features[key][1], vec_features[key][3])\n",
    "    log_reg_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7440\n",
      "F1: 0.7634\n",
      "F1: 0.3554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svc_score = [] #для SVC соответственно\n",
    "svc_model = LinearSVC(class_weight='balanced',\n",
    "                      random_state=42)\n",
    "for key in vec_features.keys():\n",
    "    svc_model.fit(vec_features[key][0], vec_features[key][2])\n",
    "    score = get_f1(svc_model, vec_features[key][1], vec_features[key][3])\n",
    "    svc_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.3905\n",
      "F1: 0.2298\n",
      "F1: 0.2882\n"
     ]
    }
   ],
   "source": [
    "kn_score = [] #KNeighbors\n",
    "kn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "for key in vec_features.keys():\n",
    "    kn_model.fit(vec_features[key][0], vec_features[key][2])\n",
    "    score = get_f1(kn_model, vec_features[key][1], vec_features[key][3])\n",
    "    kn_score.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='3'> 3. Выводы</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(data, color='green'):\n",
    "    #этим раскрасим максимальное значение метрики \n",
    "    attr = 'background-color: {}'.format(color)\n",
    "    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n",
    "        is_max = data == data.max()\n",
    "        return [attr if v else '' for v in is_max]\n",
    "    else:  # from .apply(axis=None)\n",
    "        is_max = data == data.max().max()\n",
    "        return pd.DataFrame(np.where(is_max, attr, ''),\n",
    "                            index=data.index, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(data=log_reg_score, index=vec_features.keys(), columns=['LogisticRegression'])\n",
    "score_df['LinearSVC'] = svc_score\n",
    "score_df['KNeighbors'] = kn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_34be93a6_2cb4_11eb_848a_02420a392966row1_col1 {\n",
       "            background-color:  lightgreen;\n",
       "        }</style><table id=\"T_34be93a6_2cb4_11eb_848a_02420a392966\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >LogisticRegression</th>        <th class=\"col_heading level0 col1\" >LinearSVC</th>        <th class=\"col_heading level0 col2\" >KNeighbors</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_34be93a6_2cb4_11eb_848a_02420a392966level0_row0\" class=\"row_heading level0 row0\" >bag_of_words</th>\n",
       "                        <td id=\"T_34be93a6_2cb4_11eb_848a_02420a392966row0_col0\" class=\"data row0 col0\" >0.732796</td>\n",
       "                        <td id=\"T_34be93a6_2cb4_11eb_848a_02420a392966row0_col1\" class=\"data row0 col1\" >0.744017</td>\n",
       "                        <td id=\"T_34be93a6_2cb4_11eb_848a_02420a392966row0_col2\" class=\"data row0 col2\" >0.390458</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_34be93a6_2cb4_11eb_848a_02420a392966level0_row1\" class=\"row_heading level0 row1\" >TF-IDF</th>\n",
       "                        <td id=\"T_34be93a6_2cb4_11eb_848a_02420a392966row1_col0\" class=\"data row1 col0\" >0.761038</td>\n",
       "                        <td id=\"T_34be93a6_2cb4_11eb_848a_02420a392966row1_col1\" class=\"data row1 col1\" >0.763372</td>\n",
       "                        <td id=\"T_34be93a6_2cb4_11eb_848a_02420a392966row1_col2\" class=\"data row1 col2\" >0.22981</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_34be93a6_2cb4_11eb_848a_02420a392966level0_row2\" class=\"row_heading level0 row2\" >word2vec</th>\n",
       "                        <td id=\"T_34be93a6_2cb4_11eb_848a_02420a392966row2_col0\" class=\"data row2 col0\" >0.350408</td>\n",
       "                        <td id=\"T_34be93a6_2cb4_11eb_848a_02420a392966row2_col1\" class=\"data row2 col1\" >0.355423</td>\n",
       "                        <td id=\"T_34be93a6_2cb4_11eb_848a_02420a392966row2_col2\" class=\"data row2 col2\" >0.288165</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fd6b221a590>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df.style.apply(highlight_max, color='lightgreen', axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** Из таблицы видно что лучшее значение метрики F1 *(0.7634)* до стигается при кодировании текстов TF-IDF на модели.\n",
    "\n",
    "Возможно, word2vec дал был лучше результаты, если использовать преобученную модель на большем корпусе текстов. Скорее всего выборки недостоточно для обучения."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
